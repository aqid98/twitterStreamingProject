{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6893_HW2PartII_LDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjlYUoo3nK-9"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext,SQLContext  \n",
        "from pyspark.sql import SparkSession   \n",
        "from pyspark.ml.feature import Word2Vec,CountVectorizer, RegexTokenizer  \n",
        "from pyspark.ml.clustering import LDA, LDAModel  \n",
        "from pyspark.sql.functions import col, udf  \n",
        "from pyspark.sql.functions import split, explode, udf, lit, size, col\n",
        "from pyspark.sql.types import IntegerType,ArrayType,StringType  \n",
        "import pylab as pl  "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4jjf6wWnU3D"
      },
      "source": [
        "def to_word(termIndices):\n",
        "  words = []  \n",
        "  for termID in termIndices:\n",
        "    words.append(vocab_broadcast.value[termID])      \n",
        "  return words\n",
        "\n",
        "def preprocess(tweet):\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)\n",
        "    tweet = re.sub(r'https\\S+', '', tweet)\n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)\n",
        "    tweet = re.sub(r't.co/\\S+', '', tweet) \n",
        "    tweet = tweet.strip('[link]') \n",
        "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
        "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
        "    tweet = re.sub('[^A-Za-z0-9]+', ' ', tweet) \n",
        "    tweet = re.sub('([0-9]+)', '', tweet)\n",
        "    tweet = re.sub(r\"[^\\x00-\\x7F]+\", '', tweet)\n",
        "    return tweet\n",
        "\n",
        "def remove_empty_words(tweet):\n",
        "    tweet = [word for word in tweet if len(word)>0]\n",
        "    return tweet\n",
        "     "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbEMpBH_nrPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b5eb42-3244-4744-b101-932f661eec7d"
      },
      "source": [
        "#Load your document dataframe here\n",
        "#================your code here==================\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()\n",
        "DATA_PATH = \"gs://mk4427hw2/lda.csv\"\n",
        "\n",
        "spark_df = spark.read.csv(DATA_PATH,inferSchema=True, header=True)\n",
        "print(spark_df.count(),len(spark_df.columns))\n",
        "\n",
        "\n",
        "\n",
        "#==================================================\n",
        "spark_df.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26198 1\n",
            "+-------------------------------+\n",
            "|                          tweet|\n",
            "+-------------------------------+\n",
            "|30分だけ パスはいつもの / お...|\n",
            "|           RT @OT9TRANS: 211...|\n",
            "|           MM: Ohh everyone ...|\n",
            "|           RT @OT9TRANS: 211...|\n",
            "|           MM: Ohh everyone ...|\n",
            "|           Two full pages of...|\n",
            "|           RT @decepcionadam...|\n",
            "|           RT @OT9TRANS: 211...|\n",
            "|           MM: Ohh everyone ...|\n",
            "|           RT @iJaadee: A Ge...|\n",
            "|           @JHunterPearson H...|\n",
            "|           RT @Rebby72979221...|\n",
            "|                         MADRID|\n",
            "|                        Yes pls|\n",
            "|           RT @OT9TRANS: 211...|\n",
            "|           MM: Ohh everyone ...|\n",
            "|           Mon frère il a un...|\n",
            "|           Moi j'ai du prend...|\n",
            "|           RT @princejael01:...|\n",
            "|                            ---|\n",
            "+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M16Wh6YhoDoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f2ec75-3dd2-437d-c5f5-3c335783a4ba"
      },
      "source": [
        "#CountVectorizer\n",
        "#================your code here==================\n",
        "preprocess_tweet = udf(preprocess)\n",
        "remove_empty_words=udf(remove_empty_words)\n",
        "\n",
        "spark_df = spark_df.withColumn('processed_tweets', preprocess_tweet(spark_df['tweet']))\n",
        "spark_df = spark_df.withColumn('processed_tweets', split(spark_df['processed_tweets'], ' '))\n",
        "spark_df = spark_df.withColumn('processed_tweets', remove_empty_words(spark_df['processed_tweets']))\n",
        "tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"processed_tweets\").setOutputCol(\"words\")\n",
        "t_df = tokenizer.transform(spark_df)\n",
        "t_df = t_df.drop(\"tweets\", \"processed_tweets\")\n",
        "t_df.printSchema()\n",
        "t_df.show()\n",
        "\n",
        "cv = CountVectorizer()\n",
        "cv.setInputCol(\"words\")\n",
        "cv.setOutputCol(\"count_vectors\")\n",
        "cv_model = cv.fit(transformed_dataframe)\n",
        "cv_model.setInputCol(\"words\")\n",
        "cvResult = cv_model.transform(transformed_dataframe)\n",
        "cvResult.printSchema()\n",
        "\n",
        "#=================================================="
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- tweet: string (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+-------------------------------+--------------------+\n",
            "|                          tweet|               words|\n",
            "+-------------------------------+--------------------+\n",
            "|30分だけ パスはいつもの / お...|                  []|\n",
            "|           RT @OT9TRANS: 211...|                  []|\n",
            "|           MM: Ohh everyone ...|[ohh, everyone, s...|\n",
            "|           RT @OT9TRANS: 211...|                  []|\n",
            "|           MM: Ohh everyone ...|[ohh, everyone, s...|\n",
            "|           Two full pages of...|[two, full, pages...|\n",
            "|           RT @decepcionadam...|[trnsar, sei, mas...|\n",
            "|           RT @OT9TRANS: 211...|                  []|\n",
            "|           MM: Ohh everyone ...|[ohh, everyone, s...|\n",
            "|           RT @iJaadee: A Ge...|[generator, can, ...|\n",
            "|           @JHunterPearson H...|[happy, birthday,...|\n",
            "|           RT @Rebby72979221...|[governo, spagnol...|\n",
            "|                         MADRID|            [madrid]|\n",
            "|                        Yes pls|          [yes, pls]|\n",
            "|           RT @OT9TRANS: 211...|                  []|\n",
            "|           MM: Ohh everyone ...|[ohh, everyone, s...|\n",
            "|           Mon frère il a un...|[mon, leger, mal,...|\n",
            "|           Moi j'ai du prend...|[moi, prendre, rd...|\n",
            "|           RT @princejael01:...|[dkbk, fluff, hum...|\n",
            "|                            ---|                  []|\n",
            "+-------------------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- tweet: string (nullable = true)\n",
            " |-- words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- count_vectors: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y5pLaBZolAq"
      },
      "source": [
        "#train LDA model, cluster the documents into 10 topics \n",
        "#================your code here==================\n",
        "\n",
        "lda = LDA(featuresCol=\"count_vectors\", k=10, maxIter=100)\n",
        "ldaModel = lda.fit(cvResult)\n",
        "\n",
        "#=================================================="
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovzUq8JPow3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8a9ee2-2edf-4b12-9be3-276ac55fc594"
      },
      "source": [
        "transformed = ldaModel.transform(cvResult).select(\"topicDistribution\")  \n",
        "#show the weight of every topic Distribution \n",
        "transformed.show(truncate=False)  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|topicDistribution                                                                                                                                                                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                            |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                            |\n",
            "|[0.004721082464285664,0.004574028662057844,0.010136269070491847,0.004537531439626845,0.004702175581369257,0.007060978774449306,0.004498552780243024,0.004640419216197291,0.9503416236985035,0.004787338312775403]    |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                            |\n",
            "|[0.004721082464285664,0.004574028662057844,0.010136269070491847,0.004537531439626845,0.004702175581369257,0.007060978774449306,0.004498552780243024,0.004640419216197291,0.9503416236985035,0.004787338312775403]    |\n",
            "|[0.005089329428005212,0.00493080534555392,0.3626480513459922,0.004891461318646304,0.005068947793305456,0.007611496172308215,0.004849442302352966,0.00500237439733792,0.594747338627927,0.005160753268570881]         |\n",
            "|[0.005519883249489975,0.005347948142919763,0.01184804833458018,0.005305275639416944,0.005497777346860159,0.9388479628280021,0.005259701840569394,0.005425571888751811,0.011350481231266155,0.005597349498143458]     |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                            |\n",
            "|[0.004721082464285664,0.004574028662057844,0.010136269070491847,0.004537531439626845,0.004702175581369257,0.007060978774449306,0.004498552780243024,0.004640419216197291,0.9503416236985035,0.004787338312775403]    |\n",
            "|[0.003466838101438258,0.0033588519057772635,0.37164194373604664,0.003332050856721451,0.0034529541877124843,0.005184921327358417,0.0033034276098366927,0.0034076049954689974,0.5993359104273708,0.0035154968522689766]|\n",
            "|[0.004124248641716505,0.003995785209811603,0.00885289180480778,0.003963901923423185,0.004107731943558598,0.006168419624987235,0.003929850902604963,0.004053782746988315,0.9566212587012088,0.004182128500893083]     |\n",
            "|[0.006644047056443592,0.9228940786588635,0.014261213032806523,0.006385733064320474,0.0066174391269236,0.009936696344007439,0.006330877834855018,0.0065305285363899445,0.013662090189647426,0.006737296155742548]     |\n",
            "|[0.03584182832162471,0.03472541546098438,0.07700204589988888,0.034448333388074785,0.0356982897886084,0.6028561627035035,0.03415241260806714,0.03522944367959855,0.07370123428413324,0.03634483386551655]             |\n",
            "|[0.02313387615935192,0.02241329471151782,0.0498757803585782,0.022234453895288706,0.023041230147130786,0.03459847780324483,0.022043453728039106,0.022738616466904837,0.7564622788788522,0.023458537851091615]         |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                            |\n",
            "|[0.004721082464285664,0.004574028662057844,0.010136269070491847,0.004537531439626845,0.004702175581369257,0.007060978774449306,0.004498552780243024,0.004640419216197291,0.9503416236985035,0.004787338312775403]    |\n",
            "|[0.007397304253777719,0.007166890592602412,0.6154815029185149,0.007109704282273167,0.007367679672299544,0.011063709790965997,0.3144031566406511,0.007270915691137653,0.01523801619111322,0.007501119966664251]       |\n",
            "|[0.008343199444091284,0.008083322781911862,0.9130000541923183,0.008018824084176228,0.00830978678992856,0.012477891218347926,0.00794994014801105,0.00820064959810494,0.017156043526368077,0.008460288216741706]       |\n",
            "|[0.007397304065400628,0.007166890445278413,0.015877776353182053,0.007109704182947905,0.007367679512374353,0.011063242815427496,0.007048629839994104,0.9142545966345696,0.015213058044042334,0.007501118106783215]    |\n",
            "|[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]                                                                                                                                                                            |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz6D0Tllo5bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e7bd20-78b0-42b0-8a39-da35e24cddcd"
      },
      "source": [
        "#The higher ll is, the lower lp is, the better model is.\n",
        "ll = ldaModel.logLikelihood(cvResult)  \n",
        "lp = ldaModel.logPerplexity(cvResult)\n",
        "print(\"ll: \", ll)\n",
        "print(\"lp: \", lp)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ll:  -980135.9996346916\n",
            "lp:  8.024561572879858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ_Ukzz4sS69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0c9cd9-932f-4959-a974-aeba93ae1450"
      },
      "source": [
        "# Output topics. Each is a distribution over words (matching word count vectors)\n",
        "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())+ \" words):\")\n",
        "topics = ldaModel.topicsMatrix()\n",
        "print(topics)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned topics (as distributions over vocab of 19704 words):\n",
            "DenseMatrix([[2.74651855e-01, 1.53059382e-01, 6.94984291e+02, ...,\n",
            "              2.32997000e-01, 2.61283903e+03, 6.02679314e-01],\n",
            "             [6.40625818e-01, 2.31417702e-01, 2.97297982e+01, ...,\n",
            "              5.76406763e-01, 2.68853354e+03, 3.31350583e-01],\n",
            "             [1.80457762e-01, 5.09565691e-01, 6.65599944e+02, ...,\n",
            "              1.57678497e-01, 8.31996264e+01, 3.55277925e-01],\n",
            "             ...,\n",
            "             [1.50490028e-01, 7.45127174e-01, 1.66876113e-01, ...,\n",
            "              1.57048472e-01, 1.63981799e-01, 1.51637581e-01],\n",
            "             [1.45755485e-01, 1.55839531e-01, 1.44684878e-01, ...,\n",
            "              1.50621294e-01, 1.21741016e+00, 1.51056743e-01],\n",
            "             [1.52404359e-01, 1.51924307e-01, 5.51864175e-01, ...,\n",
            "              1.52186482e-01, 1.39570878e-01, 1.48896715e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CcoP9smKAAI"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}